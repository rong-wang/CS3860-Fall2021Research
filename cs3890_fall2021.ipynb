{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "midog2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 3890 - Fall 2021\n",
        "### Rong Wang\n",
        "The MIDOG challenge is a challenge to detect mitotic figures in medical images so that detection is robust no matter what scanner is used. This is the challenge of domain generalization in microscopy images. <br>\n",
        "This is my attempt to adapt MIDOG team's baseline domain adversarial model to a Google Colab. <br>\n",
        "Currently, this code does not successfully distinguish 'hard negatives' and 'mitotic figures' due to bugs within the code. <br>\n",
        "Much of the code is from the MIDOG team's baseline domain adversarial model, which can be found at https://github.com/DeepPathology/MIDOG. \n",
        "Other parts of the code are referenced from the MIDOG team's MIDOG_ObjectDetecton_101.ipynb. <br>\n",
        "The MIDOG challenge can be found here: https://imi.thi.de/midog/. "
      ],
      "metadata": {
        "id": "64QtytqTdYAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@article{marzahl2020deep,\n",
        "  title={Deep learning-based quantification of pulmonary hemosiderophages in cytology slides},\n",
        "  author={Marzahl, Christian and Aubreville, Marc and Bertram, Christof A and Stayt, Jason and Jasensky, Anne-Katherine and Bartenschlager, Florian and Fragoso-Garcia, Marco and Barton, Ann K and Elsemann, Svenja and Jabari, Samir and Jens, Krauth and Prathmesh, Madhu and JÃ¶rn, Voigt and Jenny, Hill and Robert, Klopfleisch and Andreas, Maier },\n",
        "  journal={Scientific Reports},\n",
        "  volume={10},\n",
        "  number={1},\n",
        "  pages={1--10},\n",
        "  year={2020},\n",
        "  publisher={Nature Publishing Group}\n",
        "}"
      ],
      "metadata": {
        "id": "XBtvVnkOdzpe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIWLOPOQZbbX"
      },
      "source": [
        "#@title Import some python packages { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -U plotly\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siOwGZRFZk7X"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "folder = \"MyDrive/Colab Notebooks/MIDOG/Github_Data\" #@param {type:\"string\"}\n",
        "midog_folder = Path(\"/drive\") / Path(folder)\n",
        "\n",
        "print(midog_folder)\n",
        "#@markdown Your output should contain **MIDOG.sqlite** and **MIDOG.json**:\n",
        "print(list(midog_folder.glob(\"*.*\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sPD4LLWZmTN"
      },
      "source": [
        "# Handle who slide images\n",
        "!apt-get install python3-openslide\n",
        "from openslide import open_slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va8_MartZnwZ"
      },
      "source": [
        "# Install the object detection library\n",
        "!pip install -U object-detection-fastai\n",
        "\n",
        "from object_detection_fastai.helper.wsi_loader import *\n",
        "from object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\n",
        "from object_detection_fastai.models.RetinaNet import RetinaNet\n",
        "from object_detection_fastai.callbacks.callbacks import BBMetrics, PascalVOCMetricByDistance, PascalVOCMetric, PascalVOCMetricByDistance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWQeglv0adiT"
      },
      "source": [
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks import *\n",
        "from fastai.vision.models.unet import _get_sfs_idxs\n",
        "\n",
        "# Gradient Reverse Layer\n",
        "class GradReverse(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None\n",
        "\n",
        "# export\n",
        "class LateralUpsampleMerge(nn.Module):\n",
        "\n",
        "    def __init__(self, ch, ch_lat, hook):\n",
        "        super().__init__()\n",
        "        self.hook = hook\n",
        "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_lat(self.hook.stored) + F.interpolate(x, scale_factor=2)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, size, n_domains, alpha=1.0):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.reducer = nn.Sequential(\n",
        "            nn.Conv2d(size, size, kernel_size = (3, 3), bias=False),\n",
        "            nn.BatchNorm2d(size),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(size, size//2, kernel_size = (3, 3), bias=False),\n",
        "            nn.BatchNorm2d(size//2),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(size//2, size//4, kernel_size = (3, 3), bias=False),\n",
        "            nn.BatchNorm2d(size//4),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )#.cuda()\n",
        "        self.reducer2 = nn.Linear(size//4, n_domains, bias = False)#.cuda()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = GradReverse.apply(x, self.alpha)\n",
        "        x = self.reducer(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.reducer2(x)\n",
        "        return x\n",
        "\n",
        "class RetinaNetDA(nn.Module):\n",
        "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
        "\n",
        "    def __init__(self, encoder: nn.Module, n_classes, n_domains, final_bias:float=0.,  n_conv:float=4,\n",
        "                 chs=256, n_anchors=9, flatten=True, sizes=None, imsize = (512, 512)):\n",
        "        super().__init__()\n",
        "        self.n_classes, self.flatten = n_classes, flatten\n",
        "        self.sizes = sizes\n",
        "        sfs_szs, x, hooks = self._model_sizes(encoder, size=imsize)\n",
        "        sfs_idxs = _get_sfs_idxs(sfs_szs)\n",
        "        self.encoder = encoder\n",
        "        self.outputs = hook_outputs(self.encoder[-2:-4:-1])\n",
        "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
        "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
        "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
        "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, szs[1], hook)\n",
        "                                     for szs, hook in zip(sfs_szs[-2:-4:-1], hooks[-2:-4:-1])])\n",
        "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
        "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs, n_conv=n_conv)\n",
        "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs, n_conv=n_conv)\n",
        "        self.n_domains = n_domains\n",
        "        self.d3 = Discriminator(sfs_szs[-3][1], n_domains)\n",
        "        self.d4 = Discriminator(sfs_szs[-2][1], n_domains)\n",
        "        self.d5 = Discriminator(sfs_szs[-1][1], n_domains)\n",
        "\n",
        "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
        "        layers = [self._conv2d_relu(chs, chs, bias=True) for _ in range(n_conv)]\n",
        "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
        "        layers[-1].bias.data.zero_().add_(final_bias)\n",
        "        layers[-1].weight.data.fill_(0)\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _apply_transpose(self, func, p_states, n_classes):\n",
        "        if not self.flatten:\n",
        "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
        "            return [func(p).permute(0, 2, 3, 1).view(*sz, -1, n_classes) for p, sz in zip(p_states, sizes)]\n",
        "        else:\n",
        "            return torch.cat(\n",
        "                [func(p).permute(0, 2, 3, 1).contiguous().view(p.size(0), -1, n_classes) for p in p_states], 1)\n",
        "\n",
        "    def _model_sizes(self, m: nn.Module, size:tuple=(256,256), full:bool=True) -> Tuple[Sizes,Tensor,Hooks]:\n",
        "        \"Passes a dummy input through the model to get the various sizes\"\n",
        "        hooks = hook_outputs(m)\n",
        "        ch_in = in_channels(m)\n",
        "        x = torch.zeros(1,ch_in,*size)\n",
        "        x = m.eval()(x)\n",
        "        res = [o.stored.shape for o in hooks]\n",
        "        if not full: hooks.remove()\n",
        "        return res,x,hooks if full else res\n",
        "\n",
        "    def _conv2d_relu(self, ni:int, nf:int, ks:int=3, stride:int=1,\n",
        "                    padding:int=None, bn:bool=False, bias=True) -> nn.Sequential:\n",
        "        \"Create a `conv2d` layer with `nn.ReLU` activation and optional(`bn`) `nn.BatchNorm2d`\"\n",
        "        layers = [conv2d(ni, nf, ks=ks, stride=stride, padding=padding, bias=bias), nn.ReLU()]\n",
        "        if bn: layers.append(nn.BatchNorm2d(nf))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c5 = self.encoder(x)\n",
        "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
        "        p_states.append(self.p6top7(p_states[-1]))\n",
        "        for merge in self.merges:\n",
        "            p_states = [merge(p_states[0])] + p_states\n",
        "        for i, smooth in enumerate(self.smoothers[:3]):\n",
        "            p_states[i] = smooth(p_states[i])\n",
        "        if self.sizes is not None:\n",
        "            p_states = [p_state for p_state in p_states if p_state.size()[-1] in self.sizes]\n",
        "        #d3 = self.d3(self.outputs.stored[1])\n",
        "        #d4 = self.d4(self.outputs.stored[0])\n",
        "        d5 = self.d5(c5)\n",
        "\n",
        "        return [self._apply_transpose(self.classifier, p_states, self.n_classes),\n",
        "                self._apply_transpose(self.box_regressor, p_states, 4),\n",
        "                #d3,\n",
        "                #d4,\n",
        "                d5,\n",
        "                [[p.size(2), p.size(3)] for p in p_states]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhTsPaXbIkj"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "from object_detection_fastai.helper.object_detection_helper import *\n",
        "\n",
        "class RetinaNetFocalLossDA(nn.Module):\n",
        "\n",
        "    def __init__(self, anchors: Collection[float], gamma: float = 2., alpha: float = 0.25, pad_idx: int = 0,\n",
        "                 reg_loss: LossFunction = F.smooth_l1_loss, domain_weight: float=0.001, n_domains = 2):\n",
        "        super().__init__()\n",
        "        self.gamma, self.alpha, self.pad_idx, self.reg_loss = gamma, alpha, pad_idx, reg_loss\n",
        "        self.anchors = anchors\n",
        "        self.metric_names = ['BBloss', 'focal_loss', 'domain_loss', 'total', 'acc']\n",
        "        self.domain_weight = domain_weight\n",
        "        self.n_domains = n_domains\n",
        "\n",
        "    def _unpad(self, bbox_tgt, clas_tgt):\n",
        "        i = torch.min(torch.nonzero(clas_tgt - self.pad_idx)) if sum(clas_tgt)>0 else 0\n",
        "        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:] - 1 + self.pad_idx\n",
        "\n",
        "    def _focal_loss(self, clas_pred, clas_tgt):\n",
        "        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n",
        "        ps = torch.sigmoid(clas_pred)\n",
        "        weights = Variable(encoded_tgt * (1 - ps) + (1 - encoded_tgt) * ps)\n",
        "        alphas = (1 - encoded_tgt) * self.alpha + encoded_tgt * (1 - self.alpha)\n",
        "        weights.pow_(self.gamma).mul_(alphas)\n",
        "        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n",
        "        return clas_loss\n",
        "\n",
        "    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n",
        "        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n",
        "        matches = match_anchors(self.anchors, bbox_tgt)\n",
        "        bbox_mask = matches >= 0\n",
        "        if bbox_mask.sum() != 0:\n",
        "            bbox_pred = bbox_pred[bbox_mask]\n",
        "            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n",
        "            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))\n",
        "        else:\n",
        "            bb_loss = 0.\n",
        "        matches.add_(1)\n",
        "        clas_tgt = clas_tgt + 1\n",
        "        clas_mask = matches >= 0\n",
        "        clas_pred = clas_pred[clas_mask]\n",
        "        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])\n",
        "        clas_tgt = clas_tgt[matches[clas_mask]]\n",
        "        return bb_loss, self._focal_loss(clas_pred, clas_tgt) / torch.clamp(bbox_mask.sum(), min=1.)\n",
        "\n",
        "    def domain_focal_loss(self, clas_pred, clas_tgt, alpha=0.25, gamma=2.):\n",
        "        ce_loss = F.cross_entropy(clas_pred, clas_tgt)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = torch.mul(alpha,((1 - pt) ** gamma * ce_loss))\n",
        "        return focal_loss\n",
        "\n",
        "\n",
        "    def forward(self, output, bbox_tgts, clas_tgts, domain_tgts):\n",
        "        clas_preds, bbox_preds, domain, sizes = output\n",
        "        if bbox_tgts.device != self.anchors.device:\n",
        "            self.anchors = self.anchors.to(clas_preds.device)\n",
        "        bb_loss = torch.tensor(0, dtype=torch.float32).to(clas_preds.device)\n",
        "        focal_loss = torch.tensor(0, dtype=torch.float32).to(clas_preds.device)\n",
        "        d_loss = self.domain_focal_loss(domain, domain_tgts)\n",
        "        acc = torch.true_divide(sum(torch.argmax(domain, dim=1) == domain_tgts), domain_tgts.size(0))\n",
        "        for cp, bp, ct, bt, dt in zip(clas_preds, bbox_preds, clas_tgts,bbox_tgts, domain_tgts):\n",
        "            if dt != 3:\n",
        "                bb, focal = self._one_loss(cp, bp, ct, bt)\n",
        "                bb_loss += bb\n",
        "                focal_loss += focal\n",
        "        total_loss = (bb_loss + focal_loss)/clas_tgts.size(0) - self.domain_weight * ((d_loss)/domain_tgts.size(0))\n",
        "        self.metrics = dict(zip(self.metric_names, [bb_loss / clas_tgts[domain_tgts!=3].size(0), focal_loss / clas_tgts[domain_tgts!=3].size(0),\n",
        "                                                    d_loss / domain_tgts.size(0), total_loss,\n",
        "                                                    acc]))\n",
        "        return (bb_loss + focal_loss)/clas_tgts[domain_tgts!=3].size(0) + self.domain_weight * ((d_loss)/domain_tgts.size(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efQLhs3sbLsb"
      },
      "source": [
        "from fastai.vision import *\n",
        "from fastai.callbacks import TrackerCallback\n",
        "\n",
        "class UpdateAlphaCallback(TrackerCallback):\n",
        "    def __init__(self, learn:Learner, max_epochs):\n",
        "        super().__init__(learn)\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "    def on_epoch_begin(self,epoch, **kwargs:Any):\n",
        "        p = (epoch + 1) / self.max_epochs\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "        self.learn.model.d5.alpha = alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An1OeoCcbP7R"
      },
      "source": [
        "from object_detection_fastai.callbacks.callbacks import *\n",
        "from object_detection_fastai.helper.wsi_loader import SlideObjectItemList,SlideContainer\n",
        "\n",
        "\n",
        "class DomainAdaptationItem(ItemBase):\n",
        "    def __init__(self, imagebbox):\n",
        "        self.imagebbox = imagebbox\n",
        "        self.scanner_id = imagebbox.sample_kwargs[\"domain\"]\n",
        "        self.obj = (imagebbox, self.scanner_id)\n",
        "        self.data = [imagebbox.data]\n",
        "\n",
        "    def apply_tfms(self, tfms, **kwargs):\n",
        "        self.imagebbox = self.imagebbox.apply_tfms(tfms, **kwargs)\n",
        "        self.obj = (self.imagebbox, self.scanner_id)\n",
        "        self.data = [self.imagebbox.data]\n",
        "        return self\n",
        "\n",
        "class SlideObjectCategoryListDA(ObjectCategoryList):\n",
        "\n",
        "    def get(self, i, x: int = 0, y: int = 0):\n",
        "        h, w = self.x.items[i].shape\n",
        "        bboxes, labels = self.items[i]\n",
        "\n",
        "        bboxes = np.array([box for box in bboxes]) if len(np.array(bboxes).shape) == 1 else np.array(bboxes)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        if len(labels) > 0:\n",
        "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - x\n",
        "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - y\n",
        "\n",
        "            bb_widths = (bboxes[:, 2] - bboxes[:, 0]) / 2\n",
        "            bb_heights = (bboxes[:, 3] - bboxes[:, 1]) / 2\n",
        "\n",
        "            ids = ((bboxes[:, 0] + bb_widths) > 0) \\\n",
        "                  & ((bboxes[:, 1] + bb_heights) > 0) \\\n",
        "                  & ((bboxes[:, 2] - bb_widths) < w) \\\n",
        "                  & ((bboxes[:, 3] - bb_heights) < h)\n",
        "\n",
        "            bboxes = bboxes[ids]\n",
        "            bboxes = np.clip(bboxes, 0, max(h, w))\n",
        "            bboxes = bboxes[:, [1, 0, 3, 2]]\n",
        "\n",
        "            labels = labels[ids]\n",
        "\n",
        "        if len(labels) == 0:\n",
        "            labels = np.array([0])\n",
        "            bboxes = np.array([[0, 0, 1, 1]])\n",
        "\n",
        "        image_bbox = ImageBBox.create(h, w, bboxes, labels, classes=self.classes, pad_idx=self.pad_idx)\n",
        "        image_bbox.sample_kwargs = {\"domain\":self.x.items[i].y[-1]}\n",
        "        return DomainAdaptationItem(image_bbox)\n",
        "\n",
        "    def reconstruct(self, t, x):\n",
        "        (bboxes, labels, domain) = t\n",
        "        if len((labels - self.pad_idx).nonzero()) == 0: return\n",
        "        i = (labels - self.pad_idx).nonzero().min()\n",
        "        bboxes,labels = bboxes[i:],labels[i:]\n",
        "        return ImageBBox.create(*x.size, bboxes, labels=labels, classes=self.classes, scale=False)\n",
        "\n",
        "class ObjectItemListSlideDA(SlideObjectItemList):\n",
        "\n",
        "    def open(self, fn: SlideContainer,  x: int=0, y: int=0):\n",
        "        return Image(pil2tensor(fn.get_patch(x, y) / 255., np.float32))\n",
        "\n",
        "class PascalVOCMetricByDistanceDA(PascalVOCMetric):\n",
        "\n",
        "    def __init__(self, anchors, size, metric_names: list, detect_thresh: float=0.3, nms_thresh: float=0.5\n",
        "                 , radius: float=25, images_per_batch: int=-1):\n",
        "        self.ap = 'AP'\n",
        "        self.anchors = anchors\n",
        "        self.size = size\n",
        "        self.detect_thresh = detect_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.radius = radius\n",
        "\n",
        "        self.images_per_batch = images_per_batch\n",
        "        self.metric_names_original = metric_names\n",
        "        self.metric_names = [\"{}-{}\".format(self.ap, i) for i in metric_names]\n",
        "\n",
        "        self.evaluator = Evaluator()\n",
        "        self.boundingBoxes = BoundingBoxes()\n",
        "\n",
        "\n",
        "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
        "        bbox_gt_batch, class_gt_batch, _ = last_target\n",
        "        class_pred_batch, bbox_pred_batch = last_output[:2]\n",
        "\n",
        "        self.images_per_batch = self.images_per_batch if self.images_per_batch > 0 else class_pred_batch.shape[0]\n",
        "        for bbox_gt, class_gt, clas_pred, bbox_pred in \\\n",
        "                list(zip(bbox_gt_batch, class_gt_batch, class_pred_batch, bbox_pred_batch))[: self.images_per_batch]:\n",
        "\n",
        "            bbox_pred, scores, preds = process_output(clas_pred, bbox_pred, self.anchors, self.detect_thresh)\n",
        "            if bbox_pred is None:# or len(preds) > 3 * len(bbox_gt):\n",
        "                continue\n",
        "\n",
        "            #image = np.zeros((512, 512, 3), np.uint8)\n",
        "            t_sz = torch.Tensor([(self.size, self.size)])[None].cpu()\n",
        "            bbox_pred = to_np(rescale_boxes(bbox_pred.cpu(), t_sz))\n",
        "            # change from center to top left\n",
        "            bbox_pred[:, :2] = bbox_pred[:, :2] - bbox_pred[:, 2:] / 2\n",
        "\n",
        "\n",
        "            temp_boxes = np.copy(bbox_pred)\n",
        "            temp_boxes[:, 2] = temp_boxes[:, 0] + temp_boxes[:, 2]\n",
        "            temp_boxes[:, 3] = temp_boxes[:, 1] + temp_boxes[:, 3]\n",
        "\n",
        "\n",
        "            to_keep = non_max_suppression_by_distance(temp_boxes, to_np(scores), self.radius, return_ids=True)\n",
        "            bbox_pred, preds, scores = bbox_pred[to_keep], preds[to_keep].cpu(), scores[to_keep].cpu()\n",
        "\n",
        "            bbox_gt = bbox_gt[np.nonzero(class_gt)].squeeze(dim=1).cpu()\n",
        "            class_gt = class_gt[class_gt > 0]\n",
        "            # change gt from x,y,x2,y2 -> x,y,w,h\n",
        "            bbox_gt[:, 2:] = bbox_gt[:, 2:] - bbox_gt[:, :2]\n",
        "\n",
        "            bbox_gt = to_np(rescale_boxes(bbox_gt, t_sz))\n",
        "\n",
        "\n",
        "            class_gt = to_np(class_gt) - 1\n",
        "            preds = to_np(preds)\n",
        "            scores = to_np(scores)\n",
        "\n",
        "            for box, cla in zip(bbox_gt, class_gt):\n",
        "                temp = BoundingBox(imageName=str(self.imageCounter), classId=self.metric_names_original[cla], x=box[0], y=box[1],\n",
        "                               w=box[2], h=box[3], typeCoordinates=CoordinatesType.Absolute,\n",
        "                               bbType=BBType.GroundTruth, format=BBFormat.XYWH, imgSize=(self.size,self.size))\n",
        "\n",
        "                self.boundingBoxes.addBoundingBox(temp)\n",
        "\n",
        "            # to reduce math complexity take maximal three times the number of gt boxes\n",
        "            num_boxes = len(bbox_gt) * 3\n",
        "            for box, cla, scor in list(zip(bbox_pred, preds, scores))[:num_boxes]:\n",
        "                temp = BoundingBox(imageName=str(self.imageCounter), classId=self.metric_names_original[cla], x=box[0], y=box[1],\n",
        "                                   w=box[2], h=box[3], typeCoordinates=CoordinatesType.Absolute, classConfidence=scor,\n",
        "                                   bbType=BBType.Detected, format=BBFormat.XYWH, imgSize=(self.size, self.size))\n",
        "\n",
        "                self.boundingBoxes.addBoundingBox(temp)\n",
        "\n",
        "            #image = self.boundingBoxes.drawAllBoundingBoxes(image, str(self.imageCounter))\n",
        "            self.imageCounter += 1\n",
        "\n",
        "def bb_pad_collate_da(samples:BatchSamples, pad_idx:int=0) -> Tuple[FloatTensor, Tuple[LongTensor, LongTensor]]:\n",
        "    \"Function that collect `samples` of labelled bboxes and adds padding with `pad_idx`.\"\n",
        "    if isinstance(samples[0][1], int): return data_collate(samples)\n",
        "    max_len = max([len(s[1].data[0][1]) for s in samples])\n",
        "    bboxes = torch.zeros(len(samples), max_len, 4)\n",
        "    labels = torch.zeros(len(samples), max_len).long() + pad_idx\n",
        "    scanner_ids = torch.zeros(len(samples)).long() + pad_idx\n",
        "    imgs = []\n",
        "    for i,s in enumerate(samples):\n",
        "        imgs.append(s[0].data[None])\n",
        "        # print(s[1].scanner_id)\n",
        "        # scanner_ids[i] = s[1].scanner_id \n",
        "        bbs, lbls = s[1].data[0]\n",
        "        if not (bbs.nelement() == 0):\n",
        "            bboxes[i,-len(lbls):] = bbs\n",
        "            labels[i,-len(lbls):] = tensor(lbls)\n",
        "    return torch.cat(imgs,0), (bboxes,labels,scanner_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQyPb0p4bUNg"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "# sys.path.append(os.path.abspath('../../SlideRunner'))\n",
        "# from SlideRunner.dataAccess.database import Database\n",
        "from object_detection_fastai.helper.wsi_loader import SlideContainer\n",
        "import numpy as np\n",
        "from random import *\n",
        "import json\n",
        "\n",
        "def sample_function(y, classes, size, level_dimensions, level):\n",
        "    width, height = level_dimensions[level]\n",
        "    if len(y[0]) == 0:\n",
        "        xmin, ymin = randint(0, width - size[0]), randint(0, height - size[1])\n",
        "    else:\n",
        "        if randint(0,5) < 3:\n",
        "            class_id = np.random.choice(classes, 1)[0]\n",
        "            ids = np.array(y[1]) == class_id\n",
        "            xmin, ymin, _, _ = np.array(y[0])[ids][randint(0, np.count_nonzero(ids) - 1)]\n",
        "            xmin -= randint(0,size[0])\n",
        "            ymin -= randint(0,size[1])\n",
        "            xmin, ymin = max(0, xmin), max(0, ymin)\n",
        "            xmin, ymin = min(xmin, width - size[0]), min(ymin, height - size[1])\n",
        "        else:\n",
        "            xmin, ymin = randint(0, width - size[0]), randint(0, height - size[1])\n",
        "    return xmin, ymin\n",
        "\n",
        "\n",
        "def load_images(slide_folder, annotation_file, res_level, patch_size, scanner_id, categories):\n",
        "    container = []\n",
        "    anno_dict = {1: \"mitotic figure\", 2: \"impostor\"}\n",
        "    for image in os.listdir(slide_folder):\n",
        "        if annotation_file.split(\".\")[-1] == \"json\":\n",
        "            with open(annotation_file) as f:\n",
        "                data = json.load(f)\n",
        "                image_id = [i[\"id\"] for i in data[\"images\"] if i[\"file_name\"] == image][0]\n",
        "                annotations = [anno for anno in data['annotations'] if anno[\"image_id\"] == image_id and anno[\"category_id\"] in categories]\n",
        "                bboxes = [a[\"bbox\"] for a in annotations]\n",
        "                labels = [anno_dict[a[\"category_id\"]] for a in annotations]\n",
        "                container.append(SlideContainer(os.path.join(slide_folder, image), y=[bboxes, labels, scanner_id], level=res_level, width=patch_size, height=patch_size, sample_func=sample_function))\n",
        "        elif annotation_file.split(\".\")[-1] == \"sqlite\":\n",
        "            DB = Database().open(annotation_file)\n",
        "            slideid = DB.findSlideWithFilename(image, '')\n",
        "            DB.loadIntoMemory(slideid)\n",
        "            bboxes = [DB.annotations[anno].coordinates.flatten() for anno in DB.annotations.keys() if\n",
        "                      DB.annotations[anno].deleted == 0]\n",
        "            labels = [DB.annotations[anno].agreedClass for anno in DB.annotations.keys() if\n",
        "                      DB.annotations[anno].deleted == 0]\n",
        "            container.append(SlideContainer(os.path.join(slide_folder, image), y=[bboxes, labels], level=res_level,\n",
        "                                            width=patch_size, height=patch_size))\n",
        "        else:\n",
        "            print(\"Please provide valid annotation format\")\n",
        "    return container"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdUtqTb-h9rY"
      },
      "source": [
        "def sample_function(y, classes, size, level_dimensions, level):\n",
        "    width, height = level_dimensions[level]\n",
        "    if len(y[0]) == 0:\n",
        "        return randint(0, width - size[0]), randint(0, height -size[1])\n",
        "    else:\n",
        "        #if randint(0, 5) < 2:\n",
        "        if True:\n",
        "            class_id = np.random.choice(classes, 1)[0] # select a random class\n",
        "            ids = np.array(y[1]) == class_id # filter the annotations according to the selected class\n",
        "            xmin, ymin, _, _ = np.array(y[0])[ids][randint(0, np.count_nonzero(ids) - 1)] # randomly select one of the filtered annotatons as seed for the training patch\n",
        "            \n",
        "            # To have the selected annotation not in the center of the patch and an random offset.\n",
        "            xmin += random.randint(-size[0]/2, size[0]/2) \n",
        "            ymin += random.randint(-size[1]/2, size[1]/2)\n",
        "            xmin, ymin = max(0, int(xmin - size[0] / 2)), max(0, int(ymin -size[1] / 2))\n",
        "            xmin, ymin = min(xmin, width - size[0]), min(ymin, height - size[1])\n",
        "            return xmin, ymin\n",
        "        else:\n",
        "            return randint(0, width - size[0]), randint(0, height -size[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZjOFIyiQ7O"
      },
      "source": [
        "image_folder = midog_folder / \"images\"\n",
        "\n",
        "hamamatsu_rx_ids = list(range(0, 51))\n",
        "hamamatsu_360_ids = list(range(51, 101))\n",
        "aperio_ids = list(range(101, 151))\n",
        "leica_ids = list(range(151, 201))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpliJjYTiTFj"
      },
      "source": [
        "annotation_file = midog_folder / \"MIDOG.json\"\n",
        "rows = []\n",
        "with open(annotation_file) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    #categories = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
        "    categories = {1: 'mitotic figure', 2: 'hard negative'}\n",
        "\n",
        "    for row in data[\"images\"]:\n",
        "        file_name = row[\"file_name\"]\n",
        "        image_id = row[\"id\"]\n",
        "        width = row[\"width\"]\n",
        "        height = row[\"height\"]\n",
        "\n",
        "        scanner  = \"Hamamatsu XR\"\n",
        "        if image_id in hamamatsu_360_ids:\n",
        "            scanner  = \"Hamamatsu S360\"\n",
        "        if image_id in aperio_ids:\n",
        "            scanner  = \"Aperio CS\"\n",
        "        if image_id in leica_ids:\n",
        "            scanner  = \"Leica GT450\"\n",
        "         \n",
        "        for annotation in [anno for anno in data['annotations'] if anno[\"image_id\"] == image_id]:\n",
        "            box = annotation[\"bbox\"]\n",
        "            cat = categories[annotation[\"category_id\"]]\n",
        "\n",
        "            rows.append([file_name, image_id, width, height, box, cat, scanner])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"file_name\", \"image_id\", \"width\", \"height\", \"box\", \"cat\", \"scanner\"])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj2-OIwsh-fU"
      },
      "source": [
        "#@title Select a training and validation scanner { run: \"auto\", display-mode: \"form\" }\n",
        "\n",
        "def create_wsi_container(annotations_df: pd.DataFrame):\n",
        "\n",
        "    container = []\n",
        "\n",
        "    for image_name in tqdm(annotations_df[\"file_name\"].unique()):\n",
        "\n",
        "        image_annos = annotations_df[annotations_df[\"file_name\"] == image_name]\n",
        "\n",
        "        bboxes = [box   for box   in image_annos[\"box\"]]\n",
        "        labels = [label for label in image_annos[\"cat\"]]\n",
        "\n",
        "        # container.append(SlideContainer(image_folder/image_name, y=[bboxes, labels], level=res_level,width=patch_size, height=patch_size, sample_func=sample_function))\n",
        "        container.append(SlideContainer('/drive/MyDrive/Colab Notebooks/MIDOG/MIDOG_Challenge/images/' + image_name, y=[bboxes, labels], level=res_level,width=patch_size, height=patch_size, sample_func=sample_function))\n",
        "\n",
        "    return container\n",
        "\n",
        "#@markdown Options can also be combined like:  Hamamatsu XR, Hamamatsu S360\n",
        "train_scanner = \"Hamamatsu XR\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "val_scanner = \"Hamamatsu S360\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "\n",
        "patch_size = 256 #@param [256, 512, 1024]\n",
        "res_level = 0\n",
        "\n",
        "train_annos = df[df[\"scanner\"].isin(train_scanner.split(\",\"))]\n",
        "train_container = create_wsi_container(train_annos)\n",
        "\n",
        "val_annos = df[df[\"scanner\"].isin(val_scanner.split(\",\"))]\n",
        "valid_container = create_wsi_container(val_annos)\n",
        "\n",
        "f\"Created: {len(train_container)} training WSI container and {len(valid_container)} validation WSI container\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exbGCLdab7Di"
      },
      "source": [
        "# Training\n",
        "\n",
        "from object_detection_fastai.helper.wsi_loader import *\n",
        "\n",
        "def get_y_func(x):\n",
        "    return x.y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    slide_folder = midog_folder\n",
        "    model_dir = Path(\"models\")\n",
        "\n",
        "    patch_size = 512\n",
        "    batch_size = 12 # param\n",
        "    res_level = 0\n",
        "    bs = 12\n",
        "    domain_weight = 1\n",
        "    lr = 1e-4\n",
        "    train_samples_per_scanner = 1500\n",
        "    val_samples_per_scanner = 500\n",
        "    scales = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    ratios = [1]\n",
        "    sizes = [(64, 64), (32, 32), (16, 16)]\n",
        "    num_epochs = 200\n",
        "\n",
        "    train_scanners = [[\"A\",\"B\",\"C\",\"D\"]]\n",
        "    valid_scanners = [[\"A\",\"B\",\"C\",\"D\"]]\n",
        "    annotation_json ='E:/Slides/MIDOG/MIDOG.json'\n",
        "\n",
        "\n",
        "    tfms = get_transforms(do_flip=True,\n",
        "                          flip_vert=True,\n",
        "                          max_lighting=0.5,\n",
        "                          max_zoom=2,\n",
        "                          max_warp=0.2,\n",
        "                          p_affine=0.5,\n",
        "                          p_lighting=0.5,\n",
        "                          )\n",
        "\n",
        "\n",
        "    for t_scrs, v_scrs in zip (train_scanners, valid_scanners):\n",
        "        learner_name = 'DA_RetinaNet'\n",
        "        train_images = []\n",
        "        valid_images = []\n",
        "\n",
        "        train_samples = list(np.random.choice(train_container, train_samples_per_scanner))\n",
        "        train_images.append(train_samples)\n",
        "        valid_samples = list(np.random.choice(valid_container, val_samples_per_scanner))\n",
        "        valid_images.append(valid_samples)\n",
        "\n",
        "        train_images = list(np.random.choice(train_container, train_samples_per_scanner))\n",
        "        valid_images = list(np.random.choice(valid_container, val_samples_per_scanner))\n",
        "\n",
        "        train = ObjectItemListSlide(train_images)\n",
        "        valid = ObjectItemListSlide(valid_images)\n",
        "        item_list = ItemLists(\".\", train, valid)\n",
        "        lls = item_list.label_from_func(get_y_func, label_cls=SlideObjectCategoryListDA)\n",
        "        lls = lls.transform(tfms, tfm_y=True, size=patch_size)\n",
        "        data = lls.databunch(bs=bs, collate_fn=bb_pad_collate_da, num_workers=0).normalize()\n",
        "\n",
        "        data.train_dl = data.train_dl.new(shuffle=False) #set shuffle to false so that batch always contains all 4 scanners\n",
        "        data.valid_dl = data.valid_dl.new(shuffle=False)\n",
        "        anchors = create_anchors(sizes=sizes, ratios=ratios, scales=scales)\n",
        "        crit = RetinaNetFocalLossDA(anchors, domain_weight=domain_weight, n_domains=len(t_scrs))\n",
        "        encoder = create_body(models.resnet18, True, -2)\n",
        "        # Careful: Number of anchors has to be adapted to scales\n",
        "        model = RetinaNetDA(encoder, n_classes=data.train_ds.c, n_domains=len(t_scrs), n_anchors=len(scales) * len(ratios),\n",
        "                            sizes=[size[0] for size in sizes], chs=128, final_bias=-4., n_conv=3, imsize = (patch_size, patch_size))\n",
        "        voc = PascalVOCMetricByDistanceDA(anchors, patch_size,[str(i) for i in data.train_ds.y.classes[1:]])\n",
        "\n",
        "        learn = Learner(data, model, loss_func=crit, \n",
        "                        callback_fns=[BBMetrics, ShowGraph], \n",
        "                        metrics=[voc]\n",
        "                      )\n",
        "\n",
        "        learn.path = Path(os.getcwd())\n",
        "        alpha_up = UpdateAlphaCallback(learn, num_epochs)\n",
        "        cyc_len = 1\n",
        "        max_learning_rate = 1e-3\n",
        "\n",
        "        learn.fit_one_cycle(cyc_len, max_learning_rate)\n",
        "        learn.export('{}.pkl'.format(learner_name))\n",
        "        print(\"Saved model as {}\".format(learner_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl-2BDUSPZnh"
      },
      "source": [
        "#@title Take a look a the results { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "detect_thresh = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "nms_thresh = 0.2 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "image_count=15 #@param {type:\"integer\"}\n",
        "\n",
        "show_results_side_by_side(learn, anchors, detect_thresh=detect_thresh, nms_thresh=nms_thresh, image_count=image_count)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}